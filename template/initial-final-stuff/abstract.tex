Nowadays, the capability of doing close range proximity operations between spacecrafts to either repair, refuel or deorbit end of life and nonfunctional is more and more becoming a key-enabling factor for future space missions. When approaching uncooperative spacecrafts (no markers or other specific supportive means) it is also required that the spacecraft which performing the approach must be able to perform onboard estimates the pose (\textit{i.e.}, relative position and attitude) in order to correctly move towards - and eventually grasp - the target space object. The usage of vision-based sensors for pose estimation is particularly attractive to solve the problem of pose determination due to their low volumetric and power requirements, particularly in comparison to other systems such as \acrshort{lidar}. The capability of rendering thousands of images of the the target space object is particularly important when developing vision-based techniques aiming at solving the pose problem. For example, deep learning techniques relies on large annotated data-sets of images. For what concerns terrestrial applications, there are a plethora of data-sets commonly available, however for spaceborne applications there is a general lack of such data-sets. The main reason arises from the difficulty of acquiring realistic images of the desired space object which can be considered representative of a data-set taken by a real monocular camera.
The purpose of this thesis is to try to to overcome this limitation by presenting a method based on ray tracing which let the user generate thousand of images of a desired spacecraft given its CAD model and the desired attitude, which are then used to implement and test a vision-based pose estimation algorithm.
To reach the aforesaid purposes, the first thing made is to identify how to correctly model the scene which is being represented, by identifying all the correct optical parameters to assign to the various Earth's surfaces (such as oceans, clouds and terrains) and to the spacecraft's surfaces (which can be made of composite materials, metallic materials, covered with MLI or solar panels) which are being represented. Next, is important to set the correct illumination conditions (so, simulate sunlight). To have an image as much realistic as possible, it is of crucial importance also to simulate the camera behavior in terms of aperture angle and intrinsic noise of the sensor. Once the images are generated, they can be analyzed by using the desired vision-based algorithm. For what concerns this work, the images have been used to analyze and validate an innovative algorithm meant to solve the pose initialization problem called "\acrshort{svd} algorithm".
The \acrshort{svd} algorithm tries to estimate the unknown pose of the target by matching the geometrical information about the target (in the form of its 3D CAD model), typically stored on-board the spececraft which is performing the servicing maneuvers, with 2D geometrical information extracted from the image using edge detection techniques. The "\acrshort{svd} algorithm" adds some improvements with respect to the current state of the art of edge detection techniques, in particular it offer: the weak gradient eliminator technique to select a region of interest around the target spacecraft in the image, the usage of parameters which can be scaled with distance for what concerns the edge detection procedure and a smarter organization of the features extracted from the 2D images, which stores them in "perceptual groups", which allows to obtain a sensible reduction of the matching combinations. The thesis ends by comparing the generated data-set of images against the SPEED data-set - the first publicly available machine learning set of synthetic and real spacecraft imageries - and with a preliminar validation of the \acrshort{svd} algorithm.